{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137b06bb-3430-46aa-a3e4-c75af778e022",
   "metadata": {},
   "source": [
    "# I'm brandy-new to code... what the hell am I looking at?\n",
    "\n",
    "This thing you're viewing is called a `Jupyter Notebook`. Jupyter is an acronym for the programming languages Julia, Python, and R.\n",
    "It allows you to run code from a browser, and is really useful for exploratory data science. [Here's a more thorough/complete explanation](https://www.dataquest.io/blog/jupyter-notebook-tutorial/).\n",
    "\n",
    "I personally like jupyter notebooks because:\n",
    "- Tolerant of incompleteness and bullshit wannabe-code while you work through ideas.\n",
    "- Combine concepts and explanations right along side the code for a presentation such as this.\n",
    "- Easy to try with:\n",
    "    - [Try Jupyter](https://jupyter.org/try) (click on Jupyter Notebook)\n",
    "    - [Google's Colaboratory](https://colab.google/) (click on New Notebook),\n",
    "    - A computer program/coding environment like [Anaconda](https://www.anaconda.com/download) (what I use)\n",
    "\n",
    "\n",
    "\n",
    "There will be lots of *scary-scary* code, so if you want to see the *sexy-sexy* stuff, skip to the bottom! Don't be too intimidated by the code ğŸ˜±, if you read through it, it might actually make a little sense, even if you don't know quite what it does. This code wasn't written perfectly in the form you now see it. This is a garbage sentence meant as filler, or so you think twice about skipping over parts, lest you miss the good stuff. As I was saying, there were lots of mistakes and a healthy bit of swearing. I leaned on google heavily for almost every part of the code... you don't know what you're doing? Well, neither did/do I. Nod appreciatively, sprinkle in a \"Hmm, yes, interesting\" every once in a while, and you'll fit in with the rest of us imposters ;)\n",
    "\n",
    "\n",
    "### The code below\n",
    "\n",
    "In the code cell below you'll see `!pip install ...`. the `!` allows you to run a command as if it's in the terminal/bash window, and the `pip install ...` allows you to load in a **library/package** for python... which begs the question: **What is a library?** Think of it like Photoshop, or Excel - but for python. Just allows you to do things with a dedicated tool.\n",
    "\n",
    "\n",
    "Once we've `pip install`ed it, then we tell python to `import` that library's code - which allows us to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4c53d-b98d-486a-b4e1-09e4e2213f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playwright --upgrade\n",
    "!playwright install\n",
    "import playwright\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# <- that means this is a comment, aka don't run this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a805015-d988-4f42-ab65-4a999fde9144",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ec707-3918-4258-9a3a-cde7371249aa",
   "metadata": {},
   "source": [
    "## Scraping Prelim data from https://scoring.dance\n",
    "Since there's no API for us or python to access the data programmatically, I opted to use web scraping with a library called `playwright`!\n",
    "\n",
    "Web scraping is great for accessing website data and clicking on buttons like a regular person browsing the interwebz. It can be fiddly though, as you have to tell the automated browser which things to click on. Fortunately, playwright + developer tools makes this fairly easy. A disadvantage to webscraping is that websites update frequently, so while it works when you make it, they might move a button later which breaks your code. API's usually have a set format that rarely, if ever, changes.\n",
    "\n",
    "You can see below that I'm using the `async` version of playwright. This is likely because it's what I was using before, and I was too lazy to change it. It also allows me to convert this process to async in the future (if I want it to be faster). This unfortunately means we'll have to annoyingly sprinkle our code with the `await` command before some of the code can run.\n",
    "\n",
    "[This is video I learned from](https://youtu.be/lvFAuUcowT4?si=rYiOJHC4_Gdi26Ho&t=130), it uses `selenium` instead of `playwright` as the automated browser, but the concept is the same. Don't worry too much about why his programming environment looks differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3eb2d68-552d-4898-98fc-ccf0964bd76e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in ./opt/anaconda3/lib/python3.12/site-packages (1.46.0)\n",
      "Requirement already satisfied: greenlet==3.0.3 in ./opt/anaconda3/lib/python3.12/site-packages (from playwright) (3.0.3)\n",
      "Requirement already satisfied: pyee==11.1.0 in ./opt/anaconda3/lib/python3.12/site-packages (from playwright) (11.1.0)\n",
      "Requirement already satisfied: typing-extensions in ./opt/anaconda3/lib/python3.12/site-packages (from pyee==11.1.0->playwright) (4.11.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response url='https://scoring.dance/enUS/?type=1' request=<Request url='https://scoring.dance/enUS/?type=1' method='GET'>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playwright = await async_playwright().start()                #the 'program'\n",
    "browser = await playwright.chromium.launch(headless=False)   #the 'browser' - we're using chromium\n",
    "context = await browser.new_context()                        #the 'tab'\n",
    "\n",
    "page = await context.new_page()                              #the 'tab'\n",
    "await page.goto('https://scoring.dance/enUS/?hype=1')        #goes to the site we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4508c-cb46-407c-8923-611a1d8ee83a",
   "metadata": {},
   "source": [
    "### What is `async`?\n",
    "\n",
    "* Asynchronous code essentially means **waiting in parallel** - particularly useful for I/O operaions which require waiting for another system (such as a waiting for a webpage to load)\n",
    "\n",
    "* It basically sends out a bunch of requests for pages, waits for them all to load, and then collects all the results once they're done - *as opposed to requesting each website one at a time, and waiting for the stupid thing to load before requesting the next webpage*.\n",
    "\n",
    "* `async` shouldn't be confused with `parallel processing` which is *drumroll*... **processing in parallel** - useful for processing multiple things at once. Yes, you can combine them for much faster, and more complex, code\n",
    "\n",
    "* That being said, I have not inplemented async properly here because it's more complicated to write, read, and unfuck. Ultimately it's because I'm lazy and this was good enough, your welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58e5aa-68cd-46ac-a2e3-f370e99ae161",
   "metadata": {},
   "source": [
    "This, pops up a convenient window for recording mouseclicks, which field you filled out, etc.\n",
    "(You'll have to unpause it to continue doing things in the cells below. Feel free to click the buttons and fiddle with it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b222d004-a6d5-432a-9f34-84afa286ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "await page.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29af7ce-ad29-4533-8617-acb3c82235a5",
   "metadata": {},
   "source": [
    "Below, I wanted all the a buttons/links on the page that have the text 'results'. I did this by:\n",
    "* right-click on the page > `Inspect Element` > right-click on thing you want > `Copy` > `XPath`\n",
    "* giving that to `page.query_selector_all()` (queries everything on the webpage that has that text)\n",
    "* get the `.get_attribute('href')` (aka the link path) of each element like: **/enUS/events/140/results/**\n",
    "* put `https://scoring.dance` before each of them to make it a complete link that I can use later\n",
    "\n",
    "(Note: the `_` is just an anonymous placeholder variable for each item in the list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67340e85-7ed8-4657-908f-cf0261b3e548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://scoring.dance/enUS/events/140/results/',\n",
       " 'https://scoring.dance/enUS/events/147/results/',\n",
       " 'https://scoring.dance/enUS/events/139/results/',\n",
       " 'https://scoring.dance/enUS/events/144/results/',\n",
       " 'https://scoring.dance/enUS/events/145/results/',\n",
       " 'https://scoring.dance/enUS/events/138/results/',\n",
       " 'https://scoring.dance/enUS/events/137/results/',\n",
       " 'https://scoring.dance/enUS/events/136/results/',\n",
       " 'https://scoring.dance/enUS/events/135/results/',\n",
       " 'https://scoring.dance/enUS/events/142/results/',\n",
       " 'https://scoring.dance/enUS/events/134/results/',\n",
       " 'https://scoring.dance/enUS/events/133/results/',\n",
       " 'https://scoring.dance/enUS/events/130/results/',\n",
       " 'https://scoring.dance/enUS/events/121/results/',\n",
       " 'https://scoring.dance/enUS/events/127/results/',\n",
       " 'https://scoring.dance/enUS/events/128/results/',\n",
       " 'https://scoring.dance/enUS/events/131/results/',\n",
       " 'https://scoring.dance/enUS/events/125/results/']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = await page.query_selector_all('a:has-text(\"results\")')\n",
    "event_results_urls = [f\"https://scoring.dance{await _.get_attribute('href')}\" for _ in links]\n",
    "event_results_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d021f16-7c86-4ec9-b53a-192eb71f3f16",
   "metadata": {},
   "source": [
    "### All that async now goes to waste\n",
    "Here, we go to each link in that list, and then find all the prelim buttons and get those links (using the same process as above for getting the XPath) - now we've got that whole path (**/enUS/events/140/results/2179.html**) for each of the prelim results!\n",
    "\n",
    "\n",
    "You can see that I also add a little `time.sleep(1)` on each iteration since there's only 665 links and I don't want to get blocked by the website. It means I have to wait a few minutes. I could either make something slightly more sophisticated, or be lazy and use that time go make a smoothie.... (plus, nobody's gonna see it, right? ğŸ¤¦ğŸ»â€â™‚ï¸) The `break` at the end would 'break' the loop - a leftover from when I was testing it, it'd run the first iteration/url and then stop.\n",
    "\n",
    "### Make Websites Hate you with this One Neat Trick! \n",
    "Remember how async sends out all the requests at once, and then waits for all of them to load? It would mean slamming the website/server with 665 requests at once - kinda like everybody registering for King Swing in the first 20 seconds and bringing the site down  (DDOS). That's no bueno. And it'd be all from my one IP, so... especially no good-o. I could batch things and whatnot, but smoothie was calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3dd3951-a8c9-4ab7-b087-9d84ec835e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://scoring.dance/enUS/events/140/results/2179.html',\n",
       " 'https://scoring.dance/enUS/events/140/results/2181.html',\n",
       " 'https://scoring.dance/enUS/events/140/results/2184.html',\n",
       " 'https://scoring.dance/enUS/events/140/results/2187.html',\n",
       " 'https://scoring.dance/enUS/events/140/results/2189.html',\n",
       " 'https://scoring.dance/enUS/events/140/results/2191.html',\n",
       " 'https://scoring.dance/enUS/events/147/results/2194.html',\n",
       " 'https://scoring.dance/enUS/events/147/results/2196.html',\n",
       " 'https://scoring.dance/enUS/events/147/results/2199.html',\n",
       " 'https://scoring.dance/enUS/events/147/results/2201.html',\n",
       " 'https://scoring.dance/enUS/events/147/results/2203.html',\n",
       " 'https://scoring.dance/enUS/events/139/results/2146.html',\n",
       " 'https://scoring.dance/enUS/events/139/results/2149.html',\n",
       " 'https://scoring.dance/enUS/events/139/results/2153.html',\n",
       " 'https://scoring.dance/enUS/events/139/results/2156.html',\n",
       " 'https://scoring.dance/enUS/events/139/results/2158.html',\n",
       " 'https://scoring.dance/enUS/events/139/results/2162.html']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "prelim_results_urls = []\n",
    "\n",
    "for _ in event_results_urls:\n",
    "    await page.goto(_)\n",
    "    prelims_url = await page.query_selector_all('a:has-text(\"prelim\")')\n",
    "    prelim_results_urls = prelim_results_urls + [f\"https://scoring.dance{await _.get_attribute('href')}\" for _ in prelims_url]\n",
    "    time.sleep(1)\n",
    "    # break\n",
    "\n",
    "print(len(prelim_results_urls))\n",
    "prelim_results_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb46099-779f-490e-9a7e-ef112d44db33",
   "metadata": {},
   "source": [
    "### Oh god, my eyes!\n",
    "I know, I know. Looks scary. \n",
    "\n",
    "Since I want to scrape the prelims results off each of the links/pages we've collected, I figured it'd be responsible/easier to make a function to do it. It just makes things easier to read later. And hey, if you look closesly, we've already kinda dealt with things like this before! What do you think `page.locator(...).all_inner_texts()` does? You're right! It locates that scary, scary middle part (XPath) on the webpage, and extracts the text! I do the same thing for the lead and follower tables on the page. Then I combined it all into a dictionary (`prelim_results.update(...)`) for later ease of use.\n",
    "\n",
    "Note: Be aware that I don't really know what I'm doing here, I always have to poke around and try grabbing different XPaths until one of them works. I can 'code' and I still had an issue getting those tables to work. I think some of the table extractions are still kinda fk'd (oh, hello there `,`, none of the other rows had you! Or, oh a middle name when everyone else just has first+last ğŸ™„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "702a6c39-5247-497e-81e8-a56bf61f7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl   #shhh you'll learn about this later\n",
    "\n",
    "async def get_prelim_data(prelim_result_url):\n",
    "    '''given a url, scrapes things from that webpage, returns a polars dataframe'''\n",
    "    await page.goto(prelim_result_url)\n",
    "    prelim_results = dict()\n",
    "    event = await page.locator('//*[@id=\"main\"]/div/div[1]/div').all_inner_texts()\n",
    "    division = await page.locator('//*[@id=\"anzeige\"]/h2').all_inner_texts()\n",
    "    table_leads =     page.locator('//*[@id=\"anzeige\"]/table[1]')\n",
    "    table_followers = page.locator('//*[@id=\"anzeige\"]/table[2]')\n",
    "    lead_data = await table_leads.all_inner_texts()\n",
    "    follower_data = await table_followers.all_inner_texts()\n",
    "    prelim_results.update({'competitors': lead_data[0].split('\\t')[10::9] + follower_data[0].split('\\t')[10::9],\n",
    "                           'event': event[0],\n",
    "                           'division': division[0],\n",
    "                           'source_url': prelim_result_url,\n",
    "                           'raw_lead_data': lead_data[0],\n",
    "                           'raw_follower_data': follower_data[0],\n",
    "                          })\n",
    "    return pl.DataFrame(prelim_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06e35a-1310-4cf2-91db-ec01f4445bfe",
   "metadata": {},
   "source": [
    "### Hooray, another 'for' loop!\n",
    "We see the `await` keyword - so we know I'm htting websites, and you know what that means, right kids? No, you silly gooses, not world peace, it means we get to use `time.sleep()` again! Gotta ğŸ¤« whilst sneaking around websites programmatically.\n",
    "\n",
    "For each event's prelim data (the webpage the prelim data is on), we dump it all into a list to later combine. We also have a counter after each result so it doesn't take freaking forever (there's only so many smoothies one can have). It goes pretty quick until the count is evenly divisible by 10 (`count % 10 == 0` means if there's no remainder after being divided), and then it sleeps for 2 seconds. \n",
    "\n",
    "If the loaded page doesn't have tables to extract, the `try` + `except` continues onto the next url after a little please-don't-block-me nap. \n",
    "\n",
    "665 -> **512** this seems in the ballpark if we account for some events that didn't have prelims results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37c3cab0-8102-4ef0-a3af-c26b492772f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_dfs = []\n",
    "count = 0\n",
    "\n",
    "for _ in prelim_results_urls:\n",
    "    if count % 10 == 0:\n",
    "        time.sleep(2)\n",
    "    try:\n",
    "        prelim_dfs.append(await get_prelim_data(_))\n",
    "        count += 1\n",
    "    except:\n",
    "        time.sleep(2)\n",
    "        pass\n",
    "    \n",
    "len(prelim_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611bf43-df6a-4405-8828-19066a195374",
   "metadata": {},
   "source": [
    "# Clean the data\n",
    "Now that we've gotten the data, we'll put it together and clean it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a19a6-cbcf-4e2a-bbc1-bb1f52966201",
   "metadata": {},
   "source": [
    "### Combine the datas\n",
    "Just a little concatenation of the prelim dataframes, and writing to file. Everything has been saved in my python environment's memory up to this point. If I restart things, I'd have to go through scraping that data again. Reading from csv is a nice save point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9327aca9-80f2-43f7-a45c-b81e957611c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.concat(prelim_dfs, how='diagonal').write_csv('prelim_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c454ca-d6f6-40d8-8c14-c81625f0192a",
   "metadata": {},
   "source": [
    "### Fruit of our labor\n",
    "Here's a first look at what we've got so far! So shiny! And 32,669 results!\n",
    "\n",
    "But those `raw_lead/follow_data` columns at the end look less shiny, let's fix that. \n",
    "\n",
    "Note: The `str` under each column name just means that it's a column of strings - letters/characters of any format which are very flexible to work with. As opposed to `int` (1,2,3), `float` (1.11, 2.22, 3.33), `boolean` (True/False), etc. which can be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76370837-f28f-4f1b-9b2b-0c7e8aa8f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (32_669, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>competitors</th><th>event</th><th>division</th><th>source_url</th><th>raw_lead_data</th><th>raw_follower_data</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Peter Jo&quot;</td><td>&quot;UpTown Swing 2024 reâ€¦</td><td>&quot;Newcomer Jack&amp;Jill pâ€¦</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tTJ\tIS\tOS\tAT\tÎ£\t\t\n",
       "9â€¦</td><td>&quot;\t\t\tTB\tFH\tMK\tAT\tÎ£\t\t\n",
       "3â€¦</td></tr><tr><td>&quot;Dennis La&quot;</td><td>&quot;UpTown Swing 2024 reâ€¦</td><td>&quot;Newcomer Jack&amp;Jill pâ€¦</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tTJ\tIS\tOS\tAT\tÎ£\t\t\n",
       "9â€¦</td><td>&quot;\t\t\tTB\tFH\tMK\tAT\tÎ£\t\t\n",
       "3â€¦</td></tr><tr><td>&quot;Maria Mo&quot;</td><td>&quot;UpTown Swing 2024 reâ€¦</td><td>&quot;Newcomer Jack&amp;Jill pâ€¦</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tTJ\tIS\tOS\tAT\tÎ£\t\t\n",
       "9â€¦</td><td>&quot;\t\t\tTB\tFH\tMK\tAT\tÎ£\t\t\n",
       "3â€¦</td></tr><tr><td>&quot;Shehab Ka&quot;</td><td>&quot;UpTown Swing 2024 reâ€¦</td><td>&quot;Newcomer Jack&amp;Jill pâ€¦</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tTJ\tIS\tOS\tAT\tÎ£\t\t\n",
       "9â€¦</td><td>&quot;\t\t\tTB\tFH\tMK\tAT\tÎ£\t\t\n",
       "3â€¦</td></tr><tr><td>&quot;Susanna Ki&quot;</td><td>&quot;UpTown Swing 2024 reâ€¦</td><td>&quot;Newcomer Jack&amp;Jill pâ€¦</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tTJ\tIS\tOS\tAT\tÎ£\t\t\n",
       "9â€¦</td><td>&quot;\t\t\tTB\tFH\tMK\tAT\tÎ£\t\t\n",
       "3â€¦</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Zeljka Sp&quot;</td><td>&quot;Carnival Swing Cologâ€¦</td><td>&quot;Open J&amp;J prelim&quot;</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tSO\tME\tMT\tMH\tÎ£\t\t\n",
       "1â€¦</td><td>&quot;\t\t\tTB\tVG\tGK\tMH\tÎ£\t\t\n",
       "1â€¦</td></tr><tr><td>&quot;Sarah Ei&quot;</td><td>&quot;Carnival Swing Cologâ€¦</td><td>&quot;Open J&amp;J prelim&quot;</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tSO\tME\tMT\tMH\tÎ£\t\t\n",
       "1â€¦</td><td>&quot;\t\t\tTB\tVG\tGK\tMH\tÎ£\t\t\n",
       "1â€¦</td></tr><tr><td>&quot;Nicole Va&quot;</td><td>&quot;Carnival Swing Cologâ€¦</td><td>&quot;Open J&amp;J prelim&quot;</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tSO\tME\tMT\tMH\tÎ£\t\t\n",
       "1â€¦</td><td>&quot;\t\t\tTB\tVG\tGK\tMH\tÎ£\t\t\n",
       "1â€¦</td></tr><tr><td>&quot;Maike Be&quot;</td><td>&quot;Carnival Swing Cologâ€¦</td><td>&quot;Open J&amp;J prelim&quot;</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tSO\tME\tMT\tMH\tÎ£\t\t\n",
       "1â€¦</td><td>&quot;\t\t\tTB\tVG\tGK\tMH\tÎ£\t\t\n",
       "1â€¦</td></tr><tr><td>&quot;Rebecca Sa&quot;</td><td>&quot;Carnival Swing Cologâ€¦</td><td>&quot;Open J&amp;J prelim&quot;</td><td>&quot;https://scoring.dancâ€¦</td><td>&quot;\t\t\tSO\tME\tMT\tMH\tÎ£\t\t\n",
       "1â€¦</td><td>&quot;\t\t\tTB\tVG\tGK\tMH\tÎ£\t\t\n",
       "1â€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (32_669, 6)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ competitors â”† event          â”† division       â”† source_url     â”† raw_lead_data  â”† raw_follower_d â”‚\n",
       "â”‚ ---         â”† ---            â”† ---            â”† ---            â”† ---            â”† ata            â”‚\n",
       "â”‚ str         â”† str            â”† str            â”† str            â”† str            â”† ---            â”‚\n",
       "â”‚             â”†                â”†                â”†                â”†                â”† str            â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Peter Jo    â”† UpTown Swing   â”† Newcomer       â”† https://scorin â”† \t\t\tTJ\tIS\tOS\tAT â”† \t\t\tTB\tFH\tMK\tAT â”‚\n",
       "â”‚             â”† 2024 reâ€¦       â”† Jack&Jill pâ€¦   â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 9â€¦             â”† 3â€¦             â”‚\n",
       "â”‚ Dennis La   â”† UpTown Swing   â”† Newcomer       â”† https://scorin â”† \t\t\tTJ\tIS\tOS\tAT â”† \t\t\tTB\tFH\tMK\tAT â”‚\n",
       "â”‚             â”† 2024 reâ€¦       â”† Jack&Jill pâ€¦   â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 9â€¦             â”† 3â€¦             â”‚\n",
       "â”‚ Maria Mo    â”† UpTown Swing   â”† Newcomer       â”† https://scorin â”† \t\t\tTJ\tIS\tOS\tAT â”† \t\t\tTB\tFH\tMK\tAT â”‚\n",
       "â”‚             â”† 2024 reâ€¦       â”† Jack&Jill pâ€¦   â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 9â€¦             â”† 3â€¦             â”‚\n",
       "â”‚ Shehab Ka   â”† UpTown Swing   â”† Newcomer       â”† https://scorin â”† \t\t\tTJ\tIS\tOS\tAT â”† \t\t\tTB\tFH\tMK\tAT â”‚\n",
       "â”‚             â”† 2024 reâ€¦       â”† Jack&Jill pâ€¦   â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 9â€¦             â”† 3â€¦             â”‚\n",
       "â”‚ Susanna Ki  â”† UpTown Swing   â”† Newcomer       â”† https://scorin â”† \t\t\tTJ\tIS\tOS\tAT â”† \t\t\tTB\tFH\tMK\tAT â”‚\n",
       "â”‚             â”† 2024 reâ€¦       â”† Jack&Jill pâ€¦   â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 9â€¦             â”† 3â€¦             â”‚\n",
       "â”‚ â€¦           â”† â€¦              â”† â€¦              â”† â€¦              â”† â€¦              â”† â€¦              â”‚\n",
       "â”‚ Zeljka Sp   â”† Carnival Swing â”† Open J&J       â”† https://scorin â”† \t\t\tSO\tME\tMT\tMH â”† \t\t\tTB\tVG\tGK\tMH â”‚\n",
       "â”‚             â”† Cologâ€¦         â”† prelim         â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 1â€¦             â”† 1â€¦             â”‚\n",
       "â”‚ Sarah Ei    â”† Carnival Swing â”† Open J&J       â”† https://scorin â”† \t\t\tSO\tME\tMT\tMH â”† \t\t\tTB\tVG\tGK\tMH â”‚\n",
       "â”‚             â”† Cologâ€¦         â”† prelim         â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 1â€¦             â”† 1â€¦             â”‚\n",
       "â”‚ Nicole Va   â”† Carnival Swing â”† Open J&J       â”† https://scorin â”† \t\t\tSO\tME\tMT\tMH â”† \t\t\tTB\tVG\tGK\tMH â”‚\n",
       "â”‚             â”† Cologâ€¦         â”† prelim         â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 1â€¦             â”† 1â€¦             â”‚\n",
       "â”‚ Maike Be    â”† Carnival Swing â”† Open J&J       â”† https://scorin â”† \t\t\tSO\tME\tMT\tMH â”† \t\t\tTB\tVG\tGK\tMH â”‚\n",
       "â”‚             â”† Cologâ€¦         â”† prelim         â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 1â€¦             â”† 1â€¦             â”‚\n",
       "â”‚ Rebecca Sa  â”† Carnival Swing â”† Open J&J       â”† https://scorin â”† \t\t\tSO\tME\tMT\tMH â”† \t\t\tTB\tVG\tGK\tMH â”‚\n",
       "â”‚             â”† Cologâ€¦         â”† prelim         â”† g.dancâ€¦        â”† \tÎ£\t\t           â”† \tÎ£\t\t           â”‚\n",
       "â”‚             â”†                â”†                â”†                â”† 1â€¦             â”† 1â€¦             â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_fmt_str_lengths(20)\n",
    "\n",
    "(pl.read_csv('prelim_data.csv')\n",
    " .with_columns(pl.col('competitors').str.extract(r'^\\S+\\s\\S{2}', 0)) #first name + first 2 letters of last name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb917cf-7ab4-42d2-ae31-a57fc151698b",
   "metadata": {},
   "source": [
    "### 0-100\n",
    "\n",
    "Remember when I was talking about how a `library` was like Photoshop or Excel for python? (no, you skipped that part? after all that hard work I put into typing it?! how dare you) Well, `Polars` is ackchtually the python version of Excel! \n",
    "\n",
    "Some things to know: \n",
    "* If we 'split' a string `\"hi there\"` on spaces, it becomes a list of items/elements `['hi', 'there']` \n",
    "* `.with_columns` modifies existing columns or creates new ones.\n",
    "* `.select` is the same thing, only it selects the specified columns (and drops all other columns - so we eventually just end up with the 2 columns we want)\n",
    "* `.explode` splits 1 row with a list `[\"hi\", \"there\"]` into 2 rows.\n",
    "\n",
    "Polars is extremely powerful, and I'll use it for doing all of the heavy lifting and fun stuff, but I still use Excel regularly for perusing the file/data or as a sanity check. Sometimes the filtering options are nice with buttons to click on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efcc31d0-53dc-4692-96c1-1b9a1659a696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (22_724, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event</th><th>competitor</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Rock The Barn 2024&quot;</td><td>&quot;Jaana Sa&quot;</td></tr><tr><td>&quot;Sea Sun &amp; Swing Campâ€¦</td><td>&quot;Priscilla Be&quot;</td></tr><tr><td>&quot;City of Angels Swingâ€¦</td><td>&quot;Rachel My&quot;</td></tr><tr><td>&quot;Baltic Swing 2023&quot;</td><td>&quot;Guy Be&quot;</td></tr><tr><td>&quot;French Open 2024&quot;</td><td>&quot;Olivia We&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Nordic WCS Championsâ€¦</td><td>&quot;Pawel Sz&quot;</td></tr><tr><td>&quot;Slovenian Open 2024&quot;</td><td>&quot;Miriam Ze&quot;</td></tr><tr><td>&quot;Budafest 2024&quot;</td><td>&quot;Diana-Cristina Ia&quot;</td></tr><tr><td>&quot;Warsaw Halloween Swiâ€¦</td><td>&quot;Emil Ze&quot;</td></tr><tr><td>&quot;Augsburg Westie Statâ€¦</td><td>&quot;Michaela Ei&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (22_724, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ event                 â”† competitor        â”‚\n",
       "â”‚ ---                   â”† ---               â”‚\n",
       "â”‚ str                   â”† str               â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Rock The Barn 2024    â”† Jaana Sa          â”‚\n",
       "â”‚ Sea Sun & Swing Campâ€¦ â”† Priscilla Be      â”‚\n",
       "â”‚ City of Angels Swingâ€¦ â”† Rachel My         â”‚\n",
       "â”‚ Baltic Swing 2023     â”† Guy Be            â”‚\n",
       "â”‚ French Open 2024      â”† Olivia We         â”‚\n",
       "â”‚ â€¦                     â”† â€¦                 â”‚\n",
       "â”‚ Nordic WCS Championsâ€¦ â”† Pawel Sz          â”‚\n",
       "â”‚ Slovenian Open 2024   â”† Miriam Ze         â”‚\n",
       "â”‚ Budafest 2024         â”† Diana-Cristina Ia â”‚\n",
       "â”‚ Warsaw Halloween Swiâ€¦ â”† Emil Ze           â”‚\n",
       "â”‚ Augsburg Westie Statâ€¦ â”† Michaela Ei       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "(pl.read_csv('prelim_data.csv')\n",
    " .with_columns(pl.col('raw_lead_data', 'raw_follower_data')            #we're modifying the raw_lead_data/raw_follower_data columns\n",
    "                 .str.split('\\t')\n",
    "                 .list.eval(pl.when(pl.element().str.contains(' '))    #when the item/element of a list has a space, its likely a name - not a score\n",
    "                              .then(pl.element())                      #so keep it\n",
    "                              .otherwise(None))\n",
    "                 .list.drop_nulls(),\n",
    "               pl.col('event').str.replace(' results', ''),            #replace so we only have the event name and year, not 'results' at the end\n",
    "              )\n",
    " .explode('raw_lead_data')\n",
    " .explode('raw_follower_data')\n",
    " .select('event',\n",
    "         competitor = pl.concat_list('raw_lead_data', 'raw_follower_data')     #make new column 'competitor with both columns of names\n",
    "        )\n",
    " .explode('competitor')\n",
    " .with_columns(pl.col('competitor').str.extract(r'^\\S+\\s\\S{2}', 0))    #first name + first 2 letters of last name\n",
    " .unique()\n",
    " .write_csv('competitor_event.csv')\n",
    ")\n",
    "\n",
    "pl.read_csv('competitor_event.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bf946-9bf7-40cb-bc27-dbd881458ebb",
   "metadata": {},
   "source": [
    "### Almost pretty picture time!\n",
    "Keep in mind that this is still just the Scoring data, not the WSDC holdings, but this text shit is boring, and these peepers are hungry for visual stimulation.\n",
    "\n",
    "We're gonna use another `libary` called `networkx`. This is a network graph library which deals with nodes and edges.\n",
    "* `node`: A name or entity that has a relationship to other entities in the graph, like Thomas, or Budafest.\n",
    "* `edge`: A link between nodes. We don't specify it here, simply the fact that there IS an edge is enough. But we could easily say 'attendee' is the edge\n",
    "\n",
    "\n",
    "You'll again see the `import` statements as we're , as well as a commented-out `!pip install ..` (this is so I don't have to scroll all the way to top to install/import the libraries if I restart the notebook)\n",
    "\n",
    "We then create our graph with a truly unique name `G`. We then display the list so we can peek inside and make sure everything looks as expected, with `event` / `competitor` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee88b40b-4378-44e5-ae26-ee62d3a9049c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rock The Barn 2024', 'Jaana Sa'),\n",
       " ('Sea Sun & Swing Camp 2024', 'Priscilla Be'),\n",
       " ('City of Angels Swing 2024', 'Rachel My'),\n",
       " ('Baltic Swing 2023', 'Guy Be'),\n",
       " ('French Open 2024', 'Olivia We'),\n",
       " ('Swing Resolution 2024', 'Emma Wh'),\n",
       " ('Baltic Swing 2024', 'Outi Sa'),\n",
       " ('Cologne Swing Weekend 2023', 'Vera Ni'),\n",
       " ('German Open 2023', 'Alexander Be'),\n",
       " ('City of Angels Swing 2024', 'Alejandro He'),\n",
       " ('SwingIn - Festival 2024', 'Anna-Lisa Ba'),\n",
       " ('D-Town Swing 2023', 'Remi Kh'),\n",
       " ('Rock The Barn 2024', 'Linn Jo'),\n",
       " ('Valentine Swing 2024', 'Manxi Ye'),\n",
       " ('NeverlandSwing 2024', 'Paola Lo'),\n",
       " ('Warsaw Halloween Swing 2023', 'Maxence Ma'),\n",
       " ('West in Lyon 2024', 'MaÃ¯lys De'),\n",
       " ('Scandinavian Open WCS 2023 [SNOW 2023]', 'AneÅ¾ka Ra'),\n",
       " ('Baltic Swing 2023', 'Lukasz Ma'),\n",
       " ('Nordic WCS Championships 2023', 'Esa TÃ¶')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install ipysigma polars networkx --upgrade\n",
    "\n",
    "import networkx as nx\n",
    "import polars as pl\n",
    "\n",
    "event_competitor_tuples = list(pl.read_csv('competitor_event.csv')\n",
    "                                 # .with_columns(pl.all().str.to_lowercase()\n",
    "                                 #               .str.replace_all(r'\\d', '')\n",
    "                                 #               .str.strip_chars())\n",
    "                                 .drop_nulls()\n",
    "                                 .iter_rows()\n",
    "                                )\n",
    "\n",
    "G = nx.from_edgelist(event_competitor_tuples)\n",
    "\n",
    "event_competitor_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e810269-2c72-489b-a5e5-f607a57186ce",
   "metadata": {},
   "source": [
    "### Sweet baby jesus, finally some visuals! (push the play button)\n",
    "While you can use networkx to display networks, the visualitions are pretty crap, and absolutely useless for displaying any sizeable graph. This is where the `ipysigma` library lends a hand. We import it, and add a couple parameters to make it ultra-cool. As you can see, they're pretty self-explanatory. I had to look through the documentation examples and test them out to see which ones I like. Instead of 'white' we get a purple/pink color for text, and it doesn't really get any better when I tried other colors. No clue why that's happening, annoying, but I'll over look it.\n",
    "\n",
    "You'll also see that there are multiple Budafest nodes, one for each year. If you uncomment the lines in `event_competitor_tuples` from the previous cell, and rerun it, you'll remove all the numbers (aka years) and all the Budafest's will converge to a single node. \n",
    "- You gain: simpler/more cohesive graph\n",
    "- You lose: seeing which attendees went to one year, but not another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a3be6fa-f043-4c14-9eea-b8d96fffa55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760122ef51094d38afd746fbffc743d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sigma(nx.Graph with 7,117 nodes and 22,715 edges)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipysigma import Sigma\n",
    "\n",
    "Sigma(G, \n",
    "      node_size=G.degree, \n",
    "      node_label_size=G.degree,\n",
    "      background_color='black',\n",
    "      node_label_color='white',\n",
    "      # default_edge_type=\"curve\",\n",
    "      # start_layout=30,\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62f888-aaf1-479b-8260-946800a467d0",
   "metadata": {},
   "source": [
    "## ---TBD--- Scraping WSDC data ---TBD---\n",
    "This will be updated later. You can peek if you want to, but it's messy ğŸ˜¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8991e10f-c4c9-4fbf-ae34-935c22fff845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd00e7328e5d458db1143486ed6d309e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sigma(nx.Graph with 17,273 nodes and 52,104 edges)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install ipysigma polars networkx --upgrade\n",
    "\n",
    "import networkx as nx\n",
    "import polars as pl\n",
    "from ipysigma import Sigma\n",
    "\n",
    "\n",
    "\n",
    "G = nx.from_edgelist(list(pl.concat([pl.read_parquet('wsdc_for_gonk.parquet')\n",
    "                                    .with_columns(pl.col('dominate_data.dancer.wscid')\n",
    "                                                     .str.to_lowercase()\n",
    "                                                     .str.replace_all(r'\\d', '')\n",
    "                                                     .str.strip_chars()\n",
    "                                                     .alias('competitor'),\n",
    "                                                  pl.col('id')\n",
    "                                                   .cast(pl.String)\n",
    "                                                   .replace({k:v for k,v in pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows()})\n",
    "                                                   .str.to_lowercase()\n",
    "                                                 )\n",
    "                                   .drop('event_id', 'dominate_data.dancer.wscid'),\n",
    "                                     \n",
    "                                   pl.read_csv('competitor_event.csv')\n",
    "                                    .with_columns(pl.all().str.to_lowercase()\n",
    "                                                   .str.replace_all(r'\\d', '')\n",
    "                                                   .str.strip_chars())\n",
    "                                  ], \n",
    "                                  how='diagonal')\n",
    "                            .with_columns(eq_event = pl.col('event')\n",
    "                                                      .map_elements(lambda x: [i for i in {v.lower() for k,v in list(pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows())} if x in i], \n",
    "                                                                   return_dtype=pl.List(pl.String)\n",
    "                                                                  )\n",
    "                                         )\n",
    "                            .explode('eq_event')\n",
    "                             .with_columns(event_name = pl.when(pl.col('eq_event').is_not_null())\n",
    "                                                          .then(pl.concat_list('id', 'eq_event')\n",
    "                                                                               .list.unique()\n",
    "                                                                               .list.drop_nulls()\n",
    "                                                                               .list.sort()\n",
    "                                                               )\n",
    "                                                           .otherwise(pl.concat_list(['id', 'event'])\n",
    "                                                                        .list.unique()\n",
    "                                                                        .list.drop_nulls()\n",
    "                                                                        .list.sort()\n",
    "                                                                     )\n",
    "                                          )\n",
    "                          .explode('event_name')\n",
    "                          .select('competitor', 'event_name')\n",
    "                          .unique()\n",
    "                          .drop_nulls()\n",
    "                         .iter_rows()\n",
    "                        )\n",
    "                    )\n",
    "# G = nx.from_edgelist(list(pl.read_csv('meme_edge_list.csv').iter_rows()))\n",
    "# G.remove_node('')\n",
    "\n",
    "Sigma(G, \n",
    "      node_size=G.degree, \n",
    "      node_label_size=G.degree,\n",
    "      background_color='black',\n",
    "      node_label_color='white',\n",
    "      # default_edge_type=\"curve\",\n",
    "      # start_layout=30,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4bcb8e0-5436-41c2-a839-5dcd7d1b8be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (22_715, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event</th><th>competitor</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Rock The Barn&quot;</td><td>&quot;Jaana Sa&quot;</td></tr><tr><td>&quot;Sea Sun &amp; Swing Camp&quot;</td><td>&quot;Priscilla Be&quot;</td></tr><tr><td>&quot;City of Angels Swing&quot;</td><td>&quot;Rachel My&quot;</td></tr><tr><td>&quot;Baltic Swing&quot;</td><td>&quot;Guy Be&quot;</td></tr><tr><td>&quot;French Open&quot;</td><td>&quot;Olivia We&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Nordic WCS Championsâ€¦</td><td>&quot;Pawel Sz&quot;</td></tr><tr><td>&quot;Slovenian Open&quot;</td><td>&quot;Miriam Ze&quot;</td></tr><tr><td>&quot;Budafest&quot;</td><td>&quot;Diana-Cristina Ia&quot;</td></tr><tr><td>&quot;Warsaw Halloween Swiâ€¦</td><td>&quot;Emil Ze&quot;</td></tr><tr><td>&quot;Augsburg Westie Statâ€¦</td><td>&quot;Michaela Ei&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (22_715, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ event                 â”† competitor        â”‚\n",
       "â”‚ ---                   â”† ---               â”‚\n",
       "â”‚ str                   â”† str               â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Rock The Barn         â”† Jaana Sa          â”‚\n",
       "â”‚ Sea Sun & Swing Camp  â”† Priscilla Be      â”‚\n",
       "â”‚ City of Angels Swing  â”† Rachel My         â”‚\n",
       "â”‚ Baltic Swing          â”† Guy Be            â”‚\n",
       "â”‚ French Open           â”† Olivia We         â”‚\n",
       "â”‚ â€¦                     â”† â€¦                 â”‚\n",
       "â”‚ Nordic WCS Championsâ€¦ â”† Pawel Sz          â”‚\n",
       "â”‚ Slovenian Open        â”† Miriam Ze         â”‚\n",
       "â”‚ Budafest              â”† Diana-Cristina Ia â”‚\n",
       "â”‚ Warsaw Halloween Swiâ€¦ â”† Emil Ze           â”‚\n",
       "â”‚ Augsburg Westie Statâ€¦ â”† Michaela Ei       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pl.read_csv('competitor_event.csv')\n",
    "\n",
    "  .with_columns(pl.col('event')\n",
    "                .str.replace_all(r'\\d', '')\n",
    "                .str.strip_chars())\n",
    " # .filter(pl.col('competitor') == '')\n",
    "  .drop_nulls()\n",
    " # .iter_rows()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ed6cb8e-df9e-4471-b25b-7f81833d6349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (82_372, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>dominate_data.dancer.wscid</th><th>id</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;tina aalto&quot;</td><td>159</td></tr><tr><td>&quot;tina aalto&quot;</td><td>159</td></tr><tr><td>&quot;laila aarmo&quot;</td><td>263</td></tr><tr><td>&quot;laila aarmo&quot;</td><td>263</td></tr><tr><td>&quot;laila aarmo&quot;</td><td>222</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;tetsuya ugajin&quot;</td><td>20</td></tr><tr><td>&quot;tetsuya ugajin&quot;</td><td>104</td></tr><tr><td>&quot;tetsuya ugajin&quot;</td><td>104</td></tr><tr><td>&quot;marina ugalskaya&quot;</td><td>290</td></tr><tr><td>&quot;marina ugalskaya&quot;</td><td>290</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (82_372, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
       "â”‚ dominate_data.dancerâ€¦ â”† id  â”‚\n",
       "â”‚ ---                   â”† --- â”‚\n",
       "â”‚ str                   â”† i64 â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡\n",
       "â”‚ tina aalto            â”† 159 â”‚\n",
       "â”‚ tina aalto            â”† 159 â”‚\n",
       "â”‚ laila aarmo           â”† 263 â”‚\n",
       "â”‚ laila aarmo           â”† 263 â”‚\n",
       "â”‚ laila aarmo           â”† 222 â”‚\n",
       "â”‚ â€¦                     â”† â€¦   â”‚\n",
       "â”‚ tetsuya ugajin        â”† 20  â”‚\n",
       "â”‚ tetsuya ugajin        â”† 104 â”‚\n",
       "â”‚ tetsuya ugajin        â”† 104 â”‚\n",
       "â”‚ marina ugalskaya      â”† 290 â”‚\n",
       "â”‚ marina ugalskaya      â”† 290 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pl.read_parquet('wsdc_for_gonk.parquet')\n",
    "     .drop('event_id')\n",
    "     .with_columns(pl.col('dominate_data.dancer.wscid')\n",
    "                   .str.to_lowercase()\n",
    "                   .str.replace_all(r'\\d', '')\n",
    "                   .str.strip_chars())\n",
    "     .drop_nulls()\n",
    "     # .iter_rows()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b06564c1-0188-4011-b9fe-232fc89e1263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4th of july swing bash',\n",
       " '5280 swing dance championships \\n5280 westival',\n",
       " 'all star swingjam',\n",
       " 'american lindy hop championships',\n",
       " 'americano dance camp \\namericano dance camp',\n",
       " 'americas classic',\n",
       " 'anchor festival',\n",
       " 'anti valentines',\n",
       " 'arizona dance classic  \\narizona dance classic \\narizona dance classic (cancelled due to covid-19)',\n",
       " 'asia west coast swing open \\nasia wcs open xi \\nasia wcs open - 10th anniversary',\n",
       " 'asian wcs open swingvitation',\n",
       " 'atlanta swing classic',\n",
       " 'austin rocks',\n",
       " 'austin swing dance championships \\naustin swing dance championships (asdc)',\n",
       " 'australasian wcs & zouk champs',\n",
       " 'australian open swing dance championships',\n",
       " 'austrian open',\n",
       " 'austrian wcs spectacle',\n",
       " 'autumn swing challenge',\n",
       " 'avignon city swing',\n",
       " 'baltic swing',\n",
       " 'bavarian open wcs \\nbavarian open \\nbavarian open west coast swing championships',\n",
       " 'bay swingers',\n",
       " 'berlin swing revolution',\n",
       " 'best of the best \\nbest of the best wcs',\n",
       " 'big apple dance festival',\n",
       " 'big apple dance festival/world hustle championships \\nbig apple dance festival',\n",
       " 'boogie & blues',\n",
       " 'boogie by the bay \\nboogie by the bay',\n",
       " 'boston dance challenge']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {k:v for k,v in pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows()}\n",
    "# {v:k for k,v in pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows()}\n",
    "sorted(list(v.lower() for k,v in pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0849ada3-f3fa-4aa0-ab99-20070f9e27be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (22_715, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event</th><th>competitor</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;rock the barn&quot;</td><td>&quot;jaana sa&quot;</td></tr><tr><td>&quot;sea sun &amp; swing camp&quot;</td><td>&quot;priscilla be&quot;</td></tr><tr><td>&quot;city of angels swing&quot;</td><td>&quot;rachel my&quot;</td></tr><tr><td>&quot;baltic swing&quot;</td><td>&quot;guy be&quot;</td></tr><tr><td>&quot;french open&quot;</td><td>&quot;olivia we&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;nordic wcs championsâ€¦</td><td>&quot;pawel sz&quot;</td></tr><tr><td>&quot;slovenian open&quot;</td><td>&quot;miriam ze&quot;</td></tr><tr><td>&quot;budafest&quot;</td><td>&quot;diana-cristina ia&quot;</td></tr><tr><td>&quot;warsaw halloween swiâ€¦</td><td>&quot;emil ze&quot;</td></tr><tr><td>&quot;augsburg westie statâ€¦</td><td>&quot;michaela ei&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (22_715, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ event                 â”† competitor        â”‚\n",
       "â”‚ ---                   â”† ---               â”‚\n",
       "â”‚ str                   â”† str               â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ rock the barn         â”† jaana sa          â”‚\n",
       "â”‚ sea sun & swing camp  â”† priscilla be      â”‚\n",
       "â”‚ city of angels swing  â”† rachel my         â”‚\n",
       "â”‚ baltic swing          â”† guy be            â”‚\n",
       "â”‚ french open           â”† olivia we         â”‚\n",
       "â”‚ â€¦                     â”† â€¦                 â”‚\n",
       "â”‚ nordic wcs championsâ€¦ â”† pawel sz          â”‚\n",
       "â”‚ slovenian open        â”† miriam ze         â”‚\n",
       "â”‚ budafest              â”† diana-cristina ia â”‚\n",
       "â”‚ warsaw halloween swiâ€¦ â”† emil ze           â”‚\n",
       "â”‚ augsburg westie statâ€¦ â”† michaela ei       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pl.read_csv('competitor_event.csv')\n",
    "  .with_columns(pl.col('event')\n",
    "                .str.to_lowercase()\n",
    "                .str.replace_all(r'\\d', '')\n",
    "                .str.strip_chars(),\n",
    "                pl.col('competitor')\n",
    "                .str.to_lowercase()\n",
    "               )\n",
    "  .drop_nulls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19d2ad-fc32-4c86-a400-c8e02d798ca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(pl.concat([pl.read_parquet('wsdc_for_gonk.parquet')\n",
    "            \n",
    "            .with_columns(pl.col('dominate_data.dancer.wscid')\n",
    "                             .str.to_lowercase()\n",
    "                             .str.replace_all(r'\\d', '')\n",
    "                             .str.strip_chars()\n",
    "                             .alias('competitor'),\n",
    "                          pl.col('id')\n",
    "                           .cast(pl.String)\n",
    "                           .replace({k:v for k,v in pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows()})\n",
    "                           .str.to_lowercase()\n",
    "                         )\n",
    "           .drop('event_id', 'dominate_data.dancer.wscid')\n",
    "           , \n",
    "           \n",
    "           pl.read_csv('competitor_event.csv')\n",
    "            .with_columns(pl.all().str.to_lowercase()\n",
    "                           .str.replace_all(r'\\d', '')\n",
    "                           .str.strip_chars())\n",
    "          ], \n",
    "          how='diagonal')\n",
    "    .with_columns(eq_event = pl.col('event')\n",
    "                              .map_elements(lambda x: [i for i in {v.lower() for k,v in list(pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows())} if x in i], \n",
    "                                           return_dtype=pl.List(pl.String)\n",
    "                                          )\n",
    "                 )\n",
    "    .explode('eq_event')\n",
    "     .with_columns(event_name = pl.when(pl.col('eq_event').is_not_null())\n",
    "                                  .then(pl.concat_list('id', 'eq_event')\n",
    "                                                       .list.unique()\n",
    "                                                       .list.drop_nulls()\n",
    "                                                       .list.sort()\n",
    "                                       )\n",
    "                                   .otherwise(pl.concat_list(['id', 'event'])\n",
    "                                                .list.unique()\n",
    "                                                .list.drop_nulls()\n",
    "                                                .list.sort()\n",
    "                                             )\n",
    "                  )\n",
    "  .explode('event_name')\n",
    "  .select('competitor', 'event_name')\n",
    "  .unique()\n",
    " .iter_rows()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60d85a-fab0-4a07-a69b-fbe847222e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80776f-78d5-4f49-bce0-41908452f02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4408e-d1bd-4b57-ae18-2f5938e398a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f4210-3b3b-4e08-bbfd-44f0bcce8f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "00f59bfd-d7b4-4449-94a0-278a0b064ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scandinavian open wcs \\nscandinavian open wcs 2022 \\nscandinavian open \\nscandinavian open wcs \"snow\"'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'snow'\n",
    "[i for i in {v.lower() for k,v in pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows()} if x in i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b53395-f524-4efe-b5f2-f44ddd5770f0",
   "metadata": {},
   "source": [
    "### save to file and sanitize slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8c3c8661-666c-493b-ae6e-0389caf223df",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pl.concat([pl.read_parquet('wsdc_for_gonk.parquet')\n",
    "                                    .with_columns(pl.col('dominate_data.dancer.wscid')\n",
    "                                                     .str.to_lowercase()\n",
    "                                                     .str.replace_all(r'\\d', '')\n",
    "                                                     .str.strip_chars()\n",
    "                                                     .alias('competitor'),\n",
    "                                                  pl.col('id')\n",
    "                                                   .cast(pl.String)\n",
    "                                                   .replace({k:v for k,v in pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows()})\n",
    "                                                   .str.to_lowercase()\n",
    "                                                 )\n",
    "                                   .drop('event_id', 'dominate_data.dancer.wscid'),\n",
    "                                     \n",
    "                                   pl.read_csv('competitor_event.csv')\n",
    "                                    .with_columns(pl.all().str.to_lowercase()\n",
    "                                                   .str.replace_all(r'\\d', '')\n",
    "                                                   .str.strip_chars())\n",
    "                                  ], \n",
    "                                  how='diagonal')\n",
    "                            .with_columns(eq_event = pl.col('event')\n",
    "                                                      .map_elements(lambda x: [i for i in {v.lower() for k,v in list(pl.read_parquet('wsdc_events_and_ids.parquet').iter_rows())} if x in i], \n",
    "                                                                   return_dtype=pl.List(pl.String)\n",
    "                                                                  )\n",
    "                                         )\n",
    "                            .explode('eq_event')\n",
    "                             .with_columns(event_name = pl.when(pl.col('eq_event').is_not_null())\n",
    "                                                          .then(pl.concat_list('id', 'eq_event')\n",
    "                                                                               .list.unique()\n",
    "                                                                               .list.drop_nulls()\n",
    "                                                                               .list.sort()\n",
    "                                                               )\n",
    "                                                           .otherwise(pl.concat_list(['id', 'event'])\n",
    "                                                                        .list.unique()\n",
    "                                                                        .list.drop_nulls()\n",
    "                                                                        .list.sort()\n",
    "                                                                     )\n",
    "                                          )\n",
    "                          .explode('event_name')\n",
    "                          .select('competitor', 'event_name')\n",
    "                          .unique()\n",
    "                          .drop_nulls()\n",
    " .with_columns(pl.col('competitor').str.extract(r'^\\S+\\s\\S{2}', 0)) #first name + first 2 letters of last name\n",
    " .drop_nulls()\n",
    " .unique()\n",
    " # .write_parquet('mostly_clean_slightly_sanitized_competitor_event_edge_list.parquet')\n",
    " .write_csv('mostly_clean_slightly_sanitized_competitor_event_edge_list.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ad7ef-4064-4a6e-a983-b63e60cd5cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (Anaconda)",
   "language": "python",
   "name": "py3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
